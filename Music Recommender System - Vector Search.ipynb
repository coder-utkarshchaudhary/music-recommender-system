{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading JSON data file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['track', 'artist', 'album_type', 'album_name', 'album_artist', 'duration', 'listener_count', 'play_count', 'popularity', 'genre', 'age', 'explicit', 'thumbnail', 'song_url', 'summary', 'lyrics', 'in_movie', 'movie_name', 'youtube_link', 'language', 'sentiment', 'tempo', 'melspectrogram'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(r'data.json', 'r') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "print(raw_data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing non required fields from the json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['song_url', 'summary', 'tempo', 'melspectrogram', 'thumbnail', 'movie_name']\n",
    "for i in raw_data:\n",
    "    for j in to_remove:\n",
    "        i.pop(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['track', 'artist', 'album_type', 'album_name', 'album_artist', 'duration', 'listener_count', 'play_count', 'popularity', 'genre', 'age', 'explicit', 'thumbnail', 'lyrics', 'in_movie', 'movie_name', 'youtube_link', 'language', 'sentiment'])\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0]['explicit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracting Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(record):\n",
    "    metadata = []\n",
    "    metadata.append(0 if record['album_type'] == 'single' else 1)\n",
    "    metadata.append(record['duration']) \n",
    "    metadata.append(record['listener_count'])\n",
    "    metadata.append(record['play_count'])\n",
    "    metadata.append(record['popularity'])\n",
    "    metadata.append(record['age'])\n",
    "    metadata.append(0 if record['explicit'] == False else 1)\n",
    "    metadata.append(0 if record['in_movie'] == False else 1)\n",
    "    metadata.append(i for i in record['language'])\n",
    "\n",
    "    return np.asarray(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a separate field for genre and sentiment**\n",
    "This will be given the highest weightage since they affect the model most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_and_sentiment(model, record):\n",
    "    semantics = []\n",
    "    semantics.append(i for i in record['sentiment'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracting the lyrical features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Custom features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def extract_topics(lyrics, num_topics=5, num_words=10):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(lyrics)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    topics = []\n",
    "    for _, topic in enumerate(lda.components_):\n",
    "        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-num_words:]]\n",
    "        topics.append(words)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_tfidf(lyrics):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(lyrics)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return tfidf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def readability_scores(lyrics):\n",
    "    scores = []\n",
    "    for lyric in lyrics:\n",
    "        scores.append({\n",
    "            'flesch_reading_ease': textstat.flesch_reading_ease(lyric),\n",
    "            'smog_index': textstat.smog_index(lyric),\n",
    "            'flesch_kincaid_grade': textstat.flesch_kincaid_grade(lyric),\n",
    "            'coleman_liau_index': textstat.coleman_liau_index(lyric),\n",
    "            'automated_readability_index': textstat.automated_readability_index(lyric),\n",
    "            'dale_chall_readability_score': textstat.dale_chall_readability_score(lyric),\n",
    "            'difficult_words': textstat.difficult_words(lyric),\n",
    "            'linsear_write_formula': textstat.linsear_write_formula(lyric),\n",
    "            'gunning_fog': textstat.gunning_fog(lyric)\n",
    "        })\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_frequency(lyrics, n=2):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    ngram_matrix = vectorizer.fit_transform(lyrics)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return ngram_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def figurative_language(lyrics):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    figurative_features = []\n",
    "    for lyric in lyrics:\n",
    "        doc = nlp(lyric)\n",
    "        metaphors = [ent.text for ent in doc.ents if ent.label_ == 'METAPHOR']\n",
    "        similes = [ent.text for ent in doc.ents if ent.label_ == 'SIMILE']\n",
    "        figurative_features.append({\n",
    "            'metaphors': metaphors,\n",
    "            'similes': similes\n",
    "        })\n",
    "    return figurative_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entire lyrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyric_embeddings(model, lyrics):\n",
    "    return model.encode(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracting melspectrogram based features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube as YT\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_file(yt_link):\n",
    "    audio = YT(yt_link).streams.filter(only_audio=True).first()\n",
    "    return audio.download()\n",
    "\n",
    "def get_mel_spectrogram(audio_path):\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "    mel = librosa.power_to_db(librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=128), ref=np.max)\n",
    "    return [audio, sr, mel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using EfficientNet6B to generate melspectrogram feature maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_top_model(efficientnet_model_output):\n",
    "  # def inception_block(_input, num_filters, activation, bias, weight_regularizer, bias_regularizer):\n",
    "  #   x1 = Conv2D(num_filters, kernel_size=1, padding='same', activation=activation, use_bias = bias, kernel_regularizer=weight_regularizer, bias_regularizer = bias_regularizer)(_input)\n",
    "  #   x2 = Conv2D(num_filters, kernel_size=3, padding='same', activation=activation, use_bias = bias, kernel_regularizer=weight_regularizer, bias_regularizer = bias_regularizer)(_input)\n",
    "  #   x3 = Conv2D(num_filters, kernel_size=5, padding='same', activation=activation, use_bias = bias, kernel_regularizer=weight_regularizer, bias_regularizer = bias_regularizer)(_input)\n",
    "  #   x4 = MaxPool2D(pool_size=(3,3), strides=1, padding='same')(_input)\n",
    "  #   x4 = Conv2D(num_filters, kernel_size=1, padding='same', activation=activation, use_bias = bias, kernel_regularizer=weight_regularizer, bias_regularizer = bias_regularizer)(x4)\n",
    "  #   x = Concatenate()([x1, x2, x3, x4])\n",
    "  #   return x\n",
    "  \n",
    "  # x = inception_block(efficientnet_model_output, 64, tf.nn.tanh, True, tf.keras.regularizers.l2(3e-3), tf.keras.regularizers.l2(3e-2))\n",
    "  # x = inception_block(x, 32, tf.nn.tanh, True, tf.keras.regularizers.l2(1e-3), tf.keras.regularizers.l2(1e-2))\n",
    "  # x = inception_block(x, 16, tf.nn.tanh, True, tf.keras.regularizers.l2(1e-3), tf.keras.regularizers.l2(1e-2))\n",
    "  x = Flatten()(efficientnet_model_output)\n",
    "  x = Dense(256, activation=tf.nn.gelu)(x)\n",
    "  x = Dense(128, activation=tf.nn.gelu)(x)\n",
    "  x = Dense(32, activation=tf.nn.gelu)(x)\n",
    "  x = Dense(EMBEDDING_DIM, activation=tf.nn.tanh)(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_embedding_model(input_shape):\n",
    "  _inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "  base_model = tf.keras.EfficientNetB6(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "  for layer in base_model:\n",
    "    layer.trainable = False\n",
    "\n",
    "  efficientnet_model_output = base_model(_inputs)\n",
    "  top_model_output = cnn_top_model(efficientnet_model_output, input_shape)\n",
    "\n",
    "  model = tf.keras.Model(inputs=_inputs, outputs=top_model_output)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting audio based features from melspectrogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_features_from_melspectrogram(y, sr, mel):\n",
    "    mfccs = librosa.feature.mfcc(S=mel, sr=sr)\n",
    "    \n",
    "    spectral_centroid = librosa.feature.spectral_centroid(S=mel, sr=sr)\n",
    "    \n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(S=mel, sr=sr)\n",
    "    \n",
    "    spectral_contrast = librosa.feature.spectral_contrast(S=mel, sr=sr)\n",
    "    \n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(S=mel, sr=sr)\n",
    "    \n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    \n",
    "    chroma = librosa.feature.chroma_stft(S=mel, sr=sr)\n",
    "    \n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    \n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "\n",
    "    return [mfccs, spectral_centroid, spectral_bandwidth, spectral_contrast, spectral_rolloff, zero_crossing_rate, chroma, tonnetz, tempo]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
